{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Úkol č. 3 - předzpracování dat a binární klasifikace\n",
    "\n",
    "  * V rámci tohoto úkolu se musíte vypořádat s příznaky, které jsou různých typů.\n",
    "  * Před tím, než na nich postavíte predikční model, je třeba je nějakým způsobem převést do číselné reprezentace.\n",
    "    \n",
    "> **Úkoly jsou zadány tak, aby Vám daly prostor pro invenci. Vymyslet _jak přesně_ budete úkol řešit, je důležitou součástí zadání a originalita či nápaditost bude také hodnocena!**\n",
    "\n",
    "## Zdroj dat\n",
    "\n",
    "Budeme se zabývat predikcí přežití pasažérů Titaniku.\n",
    "K dispozici máte trénovací data v souboru **data.csv** a data na vyhodnocení v souboru **evaluation.csv**.\n",
    "\n",
    "#### Seznam příznaků:\n",
    "* survived - zda přežil, 0 = Ne, 1 = Ano, **vysvětlovaná proměnná**, kterou chcete predikovat\n",
    "* pclass - Třída lodního lístku, 1 = první, 2 = druhá, 3 = třetí\n",
    "* name - jméno\n",
    "* sex - pohlaví\n",
    "* age - věk v letech\n",
    "* sibsp\t- počet sourozenců / manželů, manželek na palubě\n",
    "* parch - počet rodičů / dětí na palubě\n",
    "* ticket - číslo lodního lístku\n",
    "* fare - cena lodního lístku\n",
    "* cabin\t- číslo kajuty\n",
    "* embarked\t- místo nalodění, C = Cherbourg, Q = Queenstown, S = Southampton\n",
    "* home.dest - Bydliště/Cíl\n",
    "\n",
    "## Pokyny k vypracování\n",
    "\n",
    "**Základní body zadání**, za jejichž (poctivé) vypracování získáte **10 bodů**:\n",
    "  * V Jupyter notebooku načtěte data ze souboru **data.csv**. Vhodným způsobem si je rozdělte na trénovací, testovací a případně i validační množinu.\n",
    "  * Projděte si jednotlivé příznaky a transformujte je do vhodné podoby pro použití ve vybraném klasifikačním modelu.\n",
    "  * Podle potřeby si můžete vytvářet nové příznaky (na základě existujících), například tedy můžete vytvořit příznak měřící délku jména. Některé příznaky můžete také úplně zahodit.\n",
    "  * Nějakým způsobem se vypořádejte s chybějícími hodnotami.\n",
    "  * Následně si vyberte vhodný klasifikační model z přednášek. Najděte vhodné hyperparametry a určete jeho přesnost (accuracy) na trénovací množině. Také určete jeho přesnost na testovací množině.\n",
    "  * Načtěte vyhodnocovací data ze souboru **evaluation.csv**. Napočítejte predikce pro tyto data (vysvětlovaná proměnná v nich již není). Vytvořte **results.csv** soubor, ve kterém tyto predikce uložíte do dvou sloupců: ID, predikce přežití. Tento soubor nahrajte do repozitáře.\n",
    "\n",
    "**Další body zadání** za případné další body  (můžete si vybrat, maximum bodů za úkol je každopádně 15 bodů):\n",
    "  * (až +5 bodů) Aplikujte všechny klasifikační modely z přednášek a určete (na základě přesnosti na validační množině), který je nejlepší. Přesnost tohoto nejlepšího modelu odhadněte pomocí testovací množiny. K predikcím na vyhodnocovacích datech využijte tento model.\n",
    "  * (až +5 bodů) Zkuste použít nějaké (alespoň dvě) netriviální metody doplňování chybějících hodnot u věku. Zaměřte na vliv těchto metod na přesnost predikce výsledného modelu. K predikcím na vyhodnocovacích datech využijte ten přístup, který Vám vyjde jako nejlepší.\n",
    "\n",
    "## Poznámky k odevzdání\n",
    "\n",
    "  * Řiďte se pokyny ze stránky https://courses.fit.cvut.cz/MI-PDM/homeworks/index.html.\n",
    "  * Odevzdejte nejen Jupyter Notebook, ale i _csv_ soubor(y) s predikcemi pro vyhodnocovací data.\n",
    "  * Opravující Vám může umožnit úkol dodělat či opravit a získat tak další body. První verze je ale důležitá a bude-li odbytá, budete za to penalizováni**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(file, y_col=\"\"):\n",
    "    data = pd.read_csv(file)\n",
    "\n",
    "    # Drop columns with many missing data\n",
    "    data = data.drop([\"cabin\", \"home.dest\"], axis=1)\n",
    "\n",
    "    # Convert sex to indexes\n",
    "    sex_idx = data[\"sex\"].astype(\"category\").cat.codes\n",
    "    data.insert(5, \"sex_idx\", sex_idx, True)\n",
    "\n",
    "    # Convert embarked to indexes\n",
    "    embarked_idx = data[\"embarked\"].astype(\"category\").cat.codes\n",
    "    data.insert(11, \"embarked_idx\", embarked_idx, True)\n",
    "\n",
    "    # Remove unnecessary column\n",
    "    data = data.drop([\"ID\", \"name\", \"sex\", \"ticket\", \"fare\", \"embarked\"], axis=1)\n",
    "\n",
    "    # Fix missing data\n",
    "    data = data.fillna(data.mean())\n",
    "    \n",
    "    X = data\n",
    "    Y = []\n",
    "    \n",
    "    if y_col != \"\":\n",
    "        Y = data[y_col]\n",
    "        X = data.drop(y_col, axis=1)\n",
    "    \n",
    "    # Standardization\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    return (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = process_data(\"data.csv\", \"survived\")\n",
    "\n",
    "# Split data to train and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "Xtrain, Xval, Ytrain, Yval = train_test_split(Xtrain, Ytrain, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare callback for showing the progress\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "class NEpochsLogger(Callback):\n",
    "    def __init__(self, display):\n",
    "        self.display = display\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.display == 0:\n",
    "            print(\"Epoch {}/{}\".format(epoch, self.params[\"epochs\"]))\n",
    "            print(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(120, activation='relu', kernel_initializer='random_normal', input_dim=Xtrain.shape[1]))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(Dense(60, activation='relu', kernel_initializer='random_normal'))\n",
    "model.add(Dense(1, activation='sigmoid', kernel_initializer=\"random_normal\"))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def train_network(patience):\n",
    "    each = int(patience / 3)\n",
    "    return model.fit(Xtrain, Ytrain,\n",
    "          validation_data=(Xval, Yval),\n",
    "          batch_size=10,\n",
    "          epochs=100,\n",
    "          verbose=False,\n",
    "          callbacks=[EarlyStopping(monitor='val_loss', patience=patience), NEpochsLogger(each)]\n",
    "         )\n",
    "\n",
    "train_network(10)\n",
    "train_network(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(Xtest, Ytest)\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xeval, _ = process_data(\"evaluation.csv\")\n",
    "Yeval = model.predict_classes(Xeval)\n",
    "Yeval.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_data = pd.read_csv(\"evaluation.csv\")\n",
    "ids = evaluation_data[\"ID\"]\n",
    "survived = pd.Series(Yeval.flatten())\n",
    "solution = pd.DataFrame({'ID': ids, 'Survived': survived})\n",
    "# This overrides the file in the repo\n",
    "# solution.to_csv(\"results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pouze pro porovnání\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(Xtrain, Ytrain)\n",
    "Xeval, _ = process_data(\"evaluation.csv\")\n",
    "print(\"Acc:\", logreg.score(Xtest, Ytest))\n",
    "logreg.predict(Xeval).sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
