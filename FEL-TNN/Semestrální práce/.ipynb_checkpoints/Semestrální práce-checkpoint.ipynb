{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = loadmat('data/letter.mat')\n",
    "# mat = loadmat('data/satimage-2.mat')\n",
    "# mat = loadmat('data/cardio.mat')\n",
    "\n",
    "df = pd.DataFrame(mat['X'])\n",
    "df['y'] = mat['y']\n",
    "\n",
    "X_train, X_test = train_test_split(df, test_size=0.2)\n",
    "X_train = X_train[X_train['y'] == 0]\n",
    "X_train.drop(['y'], axis=1)\n",
    "\n",
    "y_test = X_test['y']\n",
    "X_test.drop(['y'], axis=1)\n",
    "\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "\n",
    "print('Training data size:  ', X_train.shape)\n",
    "print('Validation data size:', X_test.shape)\n",
    "\n",
    "input_dim = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_layer = Input(shape=(input_dim,))\n",
    "# encoder = Dense(20, activation='tanh')(input_layer)\n",
    "# encoder = Dense(10, activation='tanh')(encoder)\n",
    "# encoder = Dense(5, activation='tanh')(encoder)\n",
    "# decoder = Dense(10, activation='tanh')(encoder)\n",
    "# decoder = Dense(20, activation='tanh')(decoder)\n",
    "# decoder = Dense(input_dim, activation='sigmoid')(decoder)\n",
    "# autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "# autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_undercomplete_autoencoder(n_hidden, activation='relu', summary=False):\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoder = Dense(n_hidden, activation=activation)(input_layer)\n",
    "    decoder = Dense(input_dim, activation='sigmoid')(encoder)\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "    if summary:\n",
    "        autoencoder.summary()\n",
    "\n",
    "    return autoencoder\n",
    "\n",
    "def create_sparse_autoencoder(n_hidden, activation='relu', summary=False):\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoder = Dense(n_hidden, activation=activation, activity_regularizer=regularizers.l1(10e-5))(input_layer)\n",
    "    decoder = Dense(input_dim, activation='sigmoid')(encoder)\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "    if summary:\n",
    "        autoencoder.summary()\n",
    "\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(autoencoder, data, epochs=100, batch_size=50, verbose=False):\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    history = autoencoder.fit(\n",
    "        data,\n",
    "        data,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        validation_split=0.1,\n",
    "        verbose=verbose\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe(autoencoder, data_X, data_Y, describe=False, plot=False):\n",
    "    predictions = autoencoder.predict(data_X)\n",
    "\n",
    "    mse = np.mean(np.power(data_X - predictions, 2), axis=1)\n",
    "    df_error = pd.DataFrame({'reconstruction_error': mse, 'Label': data_Y}, index=data_Y.index)\n",
    "    if describe:\n",
    "        df_error.describe()\n",
    "    if plot:\n",
    "        plt.figure()\n",
    "        plt.scatter(df_error.index, df_error['reconstruction_error'].values)\n",
    "        plt.show()\n",
    "\n",
    "    return df_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(df_error, data_Y, plot=False):\n",
    "    rec_error = df_error['reconstruction_error']\n",
    "    result = {}\n",
    "    min_threshold = 0\n",
    "    min_number_of_errors = data_Y.size\n",
    "    outliers = data_Y.index.values\n",
    "    for outlier in outliers:\n",
    "        threshold = rec_error[outlier]\n",
    "        number_of_errors = 0\n",
    "        for key in rec_error.index:\n",
    "            value = rec_error[key]\n",
    "            y = data_Y[key]\n",
    "            classification = value >= threshold\n",
    "            if classification != y:\n",
    "                number_of_errors += 1\n",
    "\n",
    "        result[threshold] = number_of_errors\n",
    "\n",
    "        if number_of_errors < min_number_of_errors:\n",
    "            min_number_of_errors = number_of_errors\n",
    "            min_threshold = threshold\n",
    "\n",
    "    if plot:\n",
    "        plt.figure()\n",
    "        plt.scatter(result.keys(), result.values())\n",
    "        plt.show()\n",
    "\n",
    "    return (min_threshold, min_number_of_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(x, number_of_errors, thresholds):\n",
    "    data1 = np.array(number_of_errors)\n",
    "    data2 = np.array(thresholds)\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('Number of hidden nodes')\n",
    "    ax1.set_ylabel('Number of misclassified points', color=color)\n",
    "    ax1.plot(x, data1, color=color, marker='o')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('Threshold', color=color)\n",
    "    ax2.plot(x, data2, color=color, marker='o')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "thresholds = []\n",
    "number_of_errors = []\n",
    "for i in range(1, input_dim):\n",
    "    print(\"Number of hidden nodes: \" + str(i))\n",
    "    model = create_undercomplete_autoencoder(n_hidden=i)\n",
    "    train(model, X_train_scaled)\n",
    "    df_error = describe(model, X_test_scaled, y_test, plot=True)\n",
    "    t, e = evaluate(df_error, y_test)\n",
    "    thresholds.append(t)\n",
    "    number_of_errors.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(range(1, input_dim), number_of_errors, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "thresholds = []\n",
    "number_of_errors = []\n",
    "for i in range(1, input_dim):\n",
    "    print(\"Number of hidden nodes: \" + str(i))\n",
    "    model = create_sparse_autoencoder(n_hidden=i)\n",
    "    train(model, X_train_scaled)\n",
    "    df_error = describe(model, X_test_scaled, y_test, plot=True)\n",
    "    t, e = evaluate(df_error, y_test)\n",
    "    thresholds.append(t)\n",
    "    number_of_errors.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(range(1, input_dim), number_of_errors, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "functions = ['relu', 'sigmoid', 'tanh', 'softmax']\n",
    "\n",
    "thresholds = []\n",
    "number_of_errors = []\n",
    "for f in functions:\n",
    "    print(\"Activation: \" + f)\n",
    "    model = create_undercomplete_autoencoder(n_hidden=6, activation=f)\n",
    "    train(model, X_train_scaled)\n",
    "    df_error = describe(model, X_test_scaled, y_test, plot=True)\n",
    "    t, e = evaluate(df_error, y_test)\n",
    "    thresholds.append(t)\n",
    "    number_of_errors.append(e)\n",
    "plot_results(functions, number_of_errors, thresholds)\n",
    "\n",
    "thresholds = []\n",
    "number_of_errors = []\n",
    "for f in functions:\n",
    "    print(\"Activation: \" + f)\n",
    "    model = create_sparse_autoencoder(n_hidden=6, activation=f)\n",
    "    train(model, X_train_scaled)\n",
    "    df_error = describe(model, X_test_scaled, y_test, plot=True)\n",
    "    t, e = evaluate(df_error, y_test)\n",
    "    thresholds.append(t)\n",
    "    number_of_errors.append(e)\n",
    "plot_results(functions, number_of_errors, thresholds)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
